from playwright.sync_api import sync_playwright, TimeoutError
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import os
from openpyxl import Workbook

def initial_shop_name(source_file: str) -> list[str]:
    """‡∏≠‡πà‡∏≤‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏£‡πâ‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå (‡∏•‡∏∞‡πÄ‡∏ß‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏á)"""
    with open(source_file, "r", encoding="utf-8") as file:
        return [shop_name.strip() for shop_name in file if shop_name.strip()]


def initial_shop_line_url(base_url: str, shop_names: list[str]) -> dict[str, str]:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á mapping {shop_name: shop_url}"""
    return {shop_name: urljoin(base_url, f"@{shop_name}") for shop_name in shop_names}


def save_list_to_file(base_dir: str, shop_name: str, filename: str, data: list[str]):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å list ‡∏Ç‡∏≠‡∏á string ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå ‡∏ñ‡πâ‡∏≤ list ‡∏ß‡πà‡∏≤‡∏á‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå"""
    if not data:
        return
    os.makedirs(base_dir, exist_ok=True)
    shop_dir = os.path.join(base_dir, f"{shop_name}_fda")
    os.makedirs(shop_dir, exist_ok=True)
    file_path = os.path.join(shop_dir, f"{shop_name}_{filename}.txt")
    with open(file_path, "w", encoding="utf-8") as f:
        f.write("\n".join(data))

def save_list_to_excel(base_dir: str, shop_name: str, filename: str, data: list[str]):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å list ‡∏Ç‡∏≠‡∏á string ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå Excel (.xlsx) ‡∏ñ‡πâ‡∏≤ list ‡∏ß‡πà‡∏≤‡∏á‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå"""
    if not data:
        return
    os.makedirs(base_dir, exist_ok=True)
    shop_dir = os.path.join(base_dir, f"{shop_name}_fda")
    os.makedirs(shop_dir, exist_ok=True)
    file_path = os.path.join(shop_dir, f"{shop_name}_{filename}.xlsx")

    wb = Workbook()
    ws = wb.active
    ws.title = "Log"

    # ‡πÉ‡∏™‡πà header
    ws.append(["Product URL / Log"])

    # ‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
    for item in data:
        ws.append([item])

    wb.save(file_path)

def scroll_to_load_all(page, scroll_pause=1500, max_scroll=50):
    """‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (lazy load)"""
    last_height = page.evaluate("document.body.scrollHeight")
    for _ in range(max_scroll):
        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        page.wait_for_timeout(scroll_pause)
        new_height = page.evaluate("document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height


def process_shop(shop_name: str, shop_url: str, base_url: str):
    """‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• FDA ‡∏Ç‡∏≠‡∏á‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏à‡∏≤‡∏Å‡∏£‡πâ‡∏≤‡∏ô‡∏Ñ‡πâ‡∏≤"""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()
        page.goto(shop_url, timeout=60000)
        page.wait_for_selector("a")

        # ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        scroll_to_load_all(page)
        page.wait_for_timeout(3000)

        # ‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤
        markup = page.content()
        soup = BeautifulSoup(markup, "html.parser")
        shop_products = soup.select("ul.grid a")
        list_product_url = [urljoin(base_url, a["href"]) for a in shop_products if a.get("href")]

        print(f"\nüõí {shop_name} ‚Üí ‡∏û‡∏ö {len(list_product_url)} ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤")

        fda_list = []
        fda_log = []

        for product_url in list_product_url:
            try:
                page.goto(product_url, timeout=60000)
                page.wait_for_selector("div")

                # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° Product license info
                try:
                    locator = page.get_by_text("Product license info", exact=False)
                    locator.scroll_into_view_if_needed()
                    locator.click(timeout=5000)
                    page.wait_for_timeout(2000)
                except TimeoutError:
                    print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏∏‡πà‡∏° license info ‚Üí {product_url}")
                    fda_log.append(product_url)
                    continue

                # ‡∏î‡∏∂‡∏á‡πÄ‡∏•‡∏Ç FDA
                fda_numbers = page.locator("div.text-sm.text-gray-500").all_text_contents()
                if fda_numbers:
                    for fda_number in fda_numbers:
                        clean_number = fda_number.strip()
                        print(f"‚úÖ {clean_number} ‚Üê {product_url}")
                        fda_list.append(clean_number)
                else:
                    print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏Ç FDA ‚Üí {product_url}")
                    fda_log.append(product_url)

            except Exception as e:
                print(f"‚ùå Error {e} ‚Üí {product_url}")
                fda_log.append(product_url)

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        save_list_to_file("fda_output", shop_name, "fda_list", fda_list)
        save_list_to_file("fda_output", shop_name, "fda_log", fda_log)
        save_list_to_excel("fda_output", shop_name, "fda_log", fda_log)

        print(f"üéâ {shop_name} ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! (FDA {len(fda_list)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£, Log {len(fda_log)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)")
        browser.close()


if __name__ == "__main__":
    base_url = "https://shop.line.me"
    shop_names = initial_shop_name("shop_name.txt")
    shop_urls = initial_shop_line_url(base_url, shop_names)

    for shop_name, shop_url in shop_urls.items():
        process_shop(shop_name, shop_url, base_url)
